# ============================================
# PLVD - Predicting Energy Consumption
# Archivo de configuración de variables de entorno
# ============================================
# 
# INSTRUCCIONES:
# 1. Copia este archivo a .env: cp .env.example .env
# 2. Edita .env con tus valores personalizados (si es necesario)
# 3. El archivo .env está en .gitignore y NO se subirá al repositorio
#
# NOTA: Todos los valores aquí son valores por defecto que funcionan
#       con la configuración estándar de docker-compose.yml
# ============================================

# ============================================
# CONFIGURACIÓN DE USUARIO Y PROYECTO
# ============================================
# Usuario y grupo para HDFS (usado para crear directorios y permisos)
HDFS_USER=amalia
HDFS_GROUP=amalia

# Nombre del proyecto (usado para paths en HDFS)
PROJECT_NAME=energy_data

# ============================================
# CONFIGURACIÓN DE KAFKA
# ============================================
# Brokers de Kafka (separados por comas para alta disponibilidad)
# Formato: broker1:puerto1,broker2:puerto2,broker3:puerto3
KAFKA_BROKER=kafka:9092,kafka2:9093,kafka3:9094

# Topic de Kafka donde se envían los datos
KAFKA_TOPIC=energy_stream

# ============================================
# CONFIGURACIÓN DE HDFS
# ============================================
# Nombre del cluster HDFS (configurado con High Availability)
# IMPORTANTE: Usar 'mycluster' para HA, NO 'namenode:9000'
HDFS_CLUSTER_NAME=mycluster

# Variables legacy (no usadas para construir paths, solo para referencia)
# El cluster usa HA, así que estas variables son principalmente informativas
HDFS_NAMENODE=namenode
HDFS_PORT=9000

# ============================================
# CONFIGURACIÓN DE HIVE
# ============================================
# URI del metastore de Hive
HIVE_METASTORE_URI=thrift://hive-metastore:9083

# Nombre de la tabla de Hive
HIVE_TABLE_NAME=energy_data

# ============================================
# CONFIGURACIÓN DEL PRODUCER
# ============================================
# Intervalo entre envío de mensajes (en segundos)
# 0 = enviar lo más rápido posible
# > 0 = pausar X segundos entre cada mensaje
PRODUCER_SEND_INTERVAL=0.5

# Ruta del dataset a procesar (relativa al directorio del producer)
PRODUCER_DATASET_PATH=data/dataset.txt

# Número máximo de reintentos si falla el envío a Kafka
PRODUCER_MAX_RETRIES=3

# ============================================
# CONFIGURACIÓN DE SPARK CONSUMER
# ============================================
# Ubicación de los checkpoints de Spark Streaming
# Puede ser local (/tmp/spark-checkpoints) o en HDFS (hdfs://mycluster/...)
SPARK_CHECKPOINT_LOCATION=/tmp/spark-checkpoints

# Intervalo de procesamiento de Spark Streaming (en segundos)
# Cada X segundos, Spark procesa un batch de datos de Kafka
SPARK_PROCESSING_INTERVAL=60

# Nombre de la aplicación Spark
SPARK_APP_NAME=EnergyDataConsumer

# ============================================
# CONFIGURACIÓN DEL DASHBOARD
# ============================================
# Puerto del dashboard (el mapeo se hace en docker-compose.yml)
DASHBOARD_PORT=5001

# Modo debug del dashboard (true/false)
DASHBOARD_DEBUG=False

# Límite de registros para la API /api/latest
DASHBOARD_LATEST_LIMIT=100

# Horas de datos para la API /api/timeseries
DASHBOARD_TIMESERIES_HOURS=24

# Path del script spark_query.py dentro del contenedor (avanzado)
# Solo cambiar si se modifica la estructura de volúmenes
SPARK_QUERY_SCRIPT_PATH=/app/consumer/spark_query.py

# ============================================
# CONFIGURACIÓN AVANZADA (Opcional)
# ============================================
# Permisos HDFS para directorios de usuario (solo números, ej: 777)
# HDFS_USER_PERMISSIONS=777

# Permisos HDFS para directorios de Hive (solo números, ej: 755)
# HDFS_HIVE_PERMISSIONS=755

# ============================================
# NOTAS IMPORTANTES
# ============================================
# 1. El archivo .env debe estar en la raíz del proyecto
# 2. Docker Compose busca el archivo en ../.env (relativo a docker/)
# 3. Los valores por defecto funcionan con la configuración estándar
# 4. No es necesario modificar este archivo a menos que quieras personalizar
# 5. Si cambias HDFS_CLUSTER_NAME, asegúrate de que coincida con docker-compose.yml
