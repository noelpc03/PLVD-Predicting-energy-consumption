# ğŸ“š RESUMEN COMPLETO - GuÃ­a RÃ¡pida de InstalaciÃ³n

## ğŸ¯ Para Empezar (Si no tienes nada instalado)

### âœ… **Lo que TIENES:**
- âœ… El proyecto descargado
- âœ… El dataset (household_power_consumption.txt)
- âœ… Una computadora con Ubuntu/Linux

### âŒ **Lo que te FALTA:**
- âŒ Docker
- âŒ Docker Compose

---

## ğŸ“ PASOS EXACTOS PARA TI

### **PASO 1: Instalar Docker (10 minutos)**

Abre una terminal y ejecuta estos comandos uno por uno:

```bash
# 1. Actualizar el sistema
sudo apt update && sudo apt upgrade -y

# 2. Instalar dependencias
sudo apt install -y ca-certificates curl gnupg lsb-release

# 3. Agregar repositorio de Docker
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# 4. Instalar Docker
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# 5. Verificar instalaciÃ³n
sudo docker --version

# DeberÃ­as ver algo como: Docker version 24.0.7, build afdd53b
```

**âœ… Docker instalado!**

---

### **PASO 2: Configurar permisos (2 minutos)**

Para no tener que usar `sudo` cada vez:

```bash
# Agregar tu usuario al grupo docker
sudo usermod -aG docker $USER

# Aplicar cambios
newgrp docker

# Verificar que funciona (SIN sudo)
docker ps
```

Si ves una tabla vacÃ­a (sin errores), Â¡funciona! âœ…

---

### **PASO 3: Verificar requisitos (1 minuto)**

```bash
# Ir a tu proyecto
cd "/home/noel/Disco D/4to_Anno/BigData/PLVD-Predicting-energy-consumption"

# Ejecutar verificaciÃ³n
./check-requirements.sh
```

**DeberÃ­as ver:**
```
âœ… Docker instalado (versiÃ³n X.X.X)
âœ… Docker estÃ¡ corriendo
âœ… Docker Compose instalado
âœ… RAM suficiente
âœ… Dataset encontrado
...
âœ… Â¡Perfecto! Tu sistema estÃ¡ listo.
```

---

### **PASO 4: Iniciar el proyecto (5 minutos)**

```bash
# Ejecutar script de inicio
./start.sh
```

Este script harÃ¡ automÃ¡ticamente:
- âœ… Construir las imÃ¡genes Docker (primera vez: ~10 min)
- âœ… Iniciar Kafka y HDFS
- âœ… Configurar directorios
- âœ… Iniciar Producer y Consumer

**Salida esperada:**
```
ğŸš€ PLVD - Inicio RÃ¡pido
========================================
âœ… Docker instalado
âœ… Dataset encontrado
ğŸ”§ Construyendo imÃ¡genes Docker...
âœ… ImÃ¡genes construidas
ğŸš€ Iniciando servicios...
âœ… Kafka estÃ¡ listo
âœ… NameNode estÃ¡ listo
âœ… Producer estÃ¡ enviando datos
âœ… Consumer estÃ¡ procesando datos
========================================
âœ… Â¡Sistema iniciado correctamente!
```

---

### **PASO 5: Verificar que funciona (2 minutos)**

#### OpciÃ³n 1: Ver logs en terminal

```bash
# Terminal 1: Ver Producer
docker logs -f producer
# Debes ver: ğŸ“¤ Enviado: {"datetime": "...", ...}

# Terminal 2: Ver Consumer (abre otra terminal)
docker logs -f spark-consumer
# Debes ver: âœ… Consumer iniciado correctamente
```

#### OpciÃ³n 2: Ver en navegador

1. **HDFS**: http://localhost:9870
   - Utilities â†’ Browse the file system
   - Navegar a `/user/amalia/energy_data/streaming/`
   - DespuÃ©s de 2-3 minutos verÃ¡s carpetas con datos

2. **Spark UI**: http://localhost:4040
   - Ver jobs en ejecuciÃ³n
   - Streaming tab muestra estadÃ­sticas

---

### **PASO 6: Consultar datos (Opcional)**

DespuÃ©s de 5 minutos de ejecuciÃ³n:

```bash
# Ver archivos en HDFS
docker exec namenode hdfs dfs -ls /user/amalia/energy_data/streaming/

# Entrar a Spark SQL
docker exec -it spark-consumer /opt/spark/bin/spark-sql --master local
```

Dentro de Spark SQL:
```sql
-- Ver registros
SELECT * FROM energy_data LIMIT 10;

-- Consumo promedio por aÃ±o
SELECT year, AVG(global_active_power) 
FROM energy_data 
GROUP BY year;

-- Salir
EXIT;
```

---

## ğŸ›‘ Para Detener Todo

```bash
cd docker
docker compose down
```

Para eliminar TODO (incluyendo datos):
```bash
docker compose down -v
```

---

## ğŸ› Si algo falla...

### Error: "Cannot connect to Docker daemon"
```bash
sudo systemctl start docker
```

### Error: "Port already in use"
```bash
# Ver quÃ© usa el puerto 9092 (ejemplo)
sudo lsof -i :9092

# Matar proceso
sudo kill -9 <PID>
```

### Error: Producer no envÃ­a datos
```bash
# Ver logs
docker logs kafka
docker logs producer

# Reiniciar
docker compose restart kafka
sleep 30
docker compose restart producer
```

### Error: No hay espacio en disco
```bash
# Limpiar Docker (imÃ¡genes y contenedores viejos)
docker system prune -a
```

---

## ğŸ“Š Tiempos Estimados

| Actividad | Primera Vez | Siguientes Veces |
|-----------|-------------|------------------|
| Instalar Docker | 10 min | - |
| Construir imÃ¡genes | 10 min | 2 min |
| Iniciar servicios | 3 min | 2 min |
| Ver primeros datos | 2 min | 2 min |
| **TOTAL** | **~25 min** | **~6 min** |

---

## âœ… Checklist RÃ¡pido

Marca cuando completes cada paso:

- [ ] Docker instalado: `docker --version`
- [ ] Permisos configurados: `docker ps` (sin sudo)
- [ ] Requisitos verificados: `./check-requirements.sh`
- [ ] Sistema iniciado: `./start.sh`
- [ ] Producer funcionando: `docker logs producer | grep Enviado`
- [ ] Consumer funcionando: `docker logs spark-consumer | grep "Consumer iniciado"`
- [ ] HDFS accesible: http://localhost:9870
- [ ] Datos en HDFS: `docker exec namenode hdfs dfs -ls /user/amalia/`

---

## ğŸ“ Archivos Importantes

- **INSTALLATION_GUIDE.md** â† GuÃ­a detallada completa
- **README.md** â† DescripciÃ³n del proyecto
- **check-requirements.sh** â† Verifica requisitos
- **start.sh** â† Inicia todo automÃ¡ticamente
- **docker/FIXES_APPLIED.md** â† Correcciones aplicadas

---

## ğŸ“ Flujo de Datos (RecapitulaciÃ³n)

```
1. Dataset (2M registros)
   â†“
2. Producer (Python) lee CSV y envÃ­a a Kafka cada 5s
   â†“
3. Kafka almacena mensajes en topic "energy_stream"
   â†“
4. Consumer (Spark) lee de Kafka cada 60s
   â†“
5. Spark transforma datos (limpia, valida, particiona)
   â†“
6. HDFS almacena en formato Parquet
   â†“
7. Hive crea tabla SQL sobre los datos
   â†“
8. AnÃ¡lisis y consultas
```

---

## ğŸ†˜ Soporte RÃ¡pido

**Problema mÃ¡s comÃºn:** "No veo datos en HDFS"
- âœ… Espera 2-3 minutos (el Consumer procesa cada 60s)
- âœ… Verifica Producer: `docker logs producer`
- âœ… Verifica Consumer: `docker logs spark-consumer`

**Segundo problema mÃ¡s comÃºn:** "Docker no inicia"
- âœ… Verifica memoria disponible: `free -h` (necesitas >6GB libres)
- âœ… Reinicia Docker: `sudo systemctl restart docker`
- âœ… Limpia espacio: `docker system prune`

---

## ğŸš€ Â¡EstÃ¡s listo!

Si llegaste hasta aquÃ­, deberÃ­as tener el sistema completo funcionando.

**PrÃ³ximos pasos:**
1. Deja correr el sistema 10-15 minutos
2. Explora HDFS en http://localhost:9870
3. Ejecuta consultas SQL con Spark
4. Revisa el cÃ³digo en `producer/` y `consumer/`

---

Ãšltima actualizaciÃ³n: 2025-10-28
Por cualquier duda, revisa los logs: `docker logs <nombre-contenedor>`
