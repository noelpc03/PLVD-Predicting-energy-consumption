#!/usr/bin/env python3
"""Script para probar consultas SQL usando la tabla registrada en Hive"""
from pyspark.sql import SparkSession

print("=" * 80)
print("üß™ PROBANDO CONSULTAS SQL CON LA TABLA REGISTRADA")
print("=" * 80)

spark = SparkSession.builder \
    .appName("TableQueryTest") \
    .config("spark.sql.warehouse.dir", "hdfs://namenode:9000/user/hive/warehouse") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
    .config("hive.metastore.uris", "thrift://hive-metastore:9083") \
    .enableHiveSupport() \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")

try:
    print("\n1Ô∏è‚É£ Consulta simple: Contar registros")
    count = spark.sql("SELECT COUNT(*) as total FROM energy_data").collect()[0]['total']
    print(f"   ‚úÖ Total: {count:,} registros")
    
    print("\n2Ô∏è‚É£ Consulta con WHERE: Registros del a√±o 2006")
    count_2006 = spark.sql("SELECT COUNT(*) as total FROM energy_data WHERE year = 2006").collect()[0]['total']
    print(f"   ‚úÖ A√±o 2006: {count_2006:,} registros")
    
    print("\n3Ô∏è‚É£ Consulta con GROUP BY: Registros por zona")
    zones = spark.sql("SELECT zone, COUNT(*) as cantidad FROM energy_data GROUP BY zone ORDER BY cantidad DESC").collect()
    print("   ‚úÖ Resultados:")
    for row in zones:
        print(f"      - {row['zone']}: {row['cantidad']:,} registros")
    
    print("\n4Ô∏è‚É£ Consulta con agregaciones: Estad√≠sticas por a√±o")
    stats = spark.sql("""
        SELECT 
            year,
            COUNT(*) as total_registros,
            ROUND(AVG(global_active_power), 2) as promedio_potencia,
            ROUND(MAX(global_active_power), 2) as max_potencia
        FROM energy_data 
        GROUP BY year 
        ORDER BY year
    """).collect()
    print("   ‚úÖ Resultados:")
    for row in stats:
        print(f"      A√±o {row['year']}: {row['total_registros']:,} registros, "
              f"Promedio: {row['promedio_potencia']} kW, "
              f"M√°ximo: {row['max_potencia']} kW")
    
    print("\n5Ô∏è‚É£ Consulta con particiones: Verificar particionado")
    partitions = spark.sql("SELECT DISTINCT year, month FROM energy_data ORDER BY year, month LIMIT 5").collect()
    print("   ‚úÖ Particiones (primeras 5):")
    for row in partitions:
        print(f"      - A√±o {row['year']}, Mes {row['month']}")
    
    print("\n" + "=" * 80)
    print("‚úÖ ¬°TODAS LAS CONSULTAS FUNCIONARON CORRECTAMENTE!")
    print("=" * 80)
    print("\nüìä La tabla 'energy_data' est√°:")
    print("   ‚úÖ Registrada en Hive Metastore")
    print("   ‚úÖ Accesible v√≠a SQL")
    print("   ‚úÖ Los datos est√°n en HDFS")
    print("   ‚úÖ Las particiones funcionan correctamente")
    
except Exception as e:
    print(f"\n‚ùå Error: {e}")
    import traceback
    traceback.print_exc()

spark.stop()

