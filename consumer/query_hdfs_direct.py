#!/usr/bin/env python3
"""Script para consultar datos de HDFS directamente desde Parquet"""
from pyspark.sql import SparkSession

# Crear Spark Session (sin Hive)
spark = SparkSession.builder \
    .appName("QueryHDFS") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")

print("=" * 80)
print("üìä CONSULTANDO DATOS DE HDFS (lectura directa Parquet)")
print("=" * 80)

# Leer directamente desde Parquet
hdfs_path = "hdfs://namenode:9000/user/amalia/energy_data/streaming"
df = spark.read.parquet(hdfs_path)

# Contar registros totales
print("\n1Ô∏è‚É£ Total de registros guardados:")
count = df.count()
print(f"   Total: {count:,} registros")

# Mostrar algunos registros de ejemplo
print("\n2Ô∏è‚É£ Primeros 10 registros:")
df.orderBy("datetime").show(10, truncate=False)

# Estad√≠sticas por a√±o
print("\n3Ô∏è‚É£ Registros por a√±o:")
df.groupBy("year").count().orderBy("year").show()

# Promedio de consumo por a√±o
print("\n4Ô∏è‚É£ Consumo promedio (global_active_power) por a√±o:")
from pyspark.sql.functions import avg, max, min, round
df.groupBy("year").agg(
    round(avg("global_active_power"), 2).alias("promedio_potencia"),
    round(max("global_active_power"), 2).alias("max_potencia"),
    round(min("global_active_power"), 2).alias("min_potencia")
).orderBy("year").show(truncate=False)

# Registros por zona
print("\n5Ô∏è‚É£ Registros por zona:")
df.groupBy("zone").count().orderBy("count", ascending=False).show()

print("\n" + "=" * 80)
print("‚úÖ Consulta completada")
print("=" * 80)

spark.stop()

