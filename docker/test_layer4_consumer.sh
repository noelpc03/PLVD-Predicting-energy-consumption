
#!/bin/bash
# test_layer4_consumer.sh - Prueba el Spark Consumer

echo "=========================================="
echo "ðŸ§ª PRUEBA CAPA 4: CONSUMO (SPARK CONSUMER)"
echo "=========================================="
echo ""

# 1. Verificar que Consumer estÃ¡ corriendo
echo "ðŸ“‹ 1. Verificando Spark Consumer..."
docker ps | grep spark-consumer
if [ $? -eq 0 ]; then
    echo "âœ… Spark Consumer estÃ¡ corriendo"
else
    echo "âŒ Spark Consumer NO estÃ¡ corriendo"
    exit 1
fi
echo ""

# 2. Verificar logs del Consumer
echo "ðŸ“‹ 2. Verificando logs del Consumer..."
echo "   Estado del consumer:"
docker logs spark-consumer --tail 30 2>&1 | grep -E "Consumer iniciado|Procesando|Leyendo|Transformando|Escribiendo|Error|Exception" | tail -10
echo ""

# 3. Verificar errores crÃ­ticos
echo "ðŸ“‹ 3. Verificando errores en logs..."
ERRORS=$(docker logs spark-consumer 2>&1 | grep -i "error\|exception\|fatal\|failed" | tail -10)
if [ -z "$ERRORS" ]; then
    echo "âœ… No se encontraron errores crÃ­ticos"
else
    echo "âš ï¸  Errores encontrados:"
    echo "$ERRORS"
fi
echo ""

# 4. Verificar descarga de dependencias
echo "ðŸ“‹ 4. Verificando descarga de dependencias..."
if docker logs spark-consumer 2>&1 | grep -q "hadoop-client-api.*jar"; then
    echo "âœ… Dependencias descargadas correctamente"
else
    echo "âš ï¸  Verificando estado de dependencias..."
    docker exec spark-consumer ls -la /root/.ivy2/jars/ | grep -E "kafka|hadoop" | head -5
fi
echo ""

# 5. Verificar conexiÃ³n Consumer -> Kafka
echo "ðŸ“‹ 5. Verificando conectividad Consumer -> Kafka..."
docker exec spark-consumer nc -zv kafka 9092 > /dev/null 2>&1
if [ $? -eq 0 ]; then
    echo "âœ… Consumer puede conectarse a Kafka"
else
    echo "âŒ Consumer NO puede conectarse a Kafka"
fi
echo ""

# 6. Verificar conexiÃ³n Consumer -> HDFS
echo "ðŸ“‹ 6. Verificando conectividad Consumer -> HDFS..."
docker exec spark-consumer nc -zv namenode 9000 > /dev/null 2>&1
if [ $? -eq 0 ]; then
    echo "âœ… Consumer puede conectarse a HDFS"
else
    echo "âŒ Consumer NO puede conectarse a HDFS"
fi
echo ""

# 7. Verificar datos en HDFS
echo "ðŸ“‹ 7. Verificando datos escritos en HDFS..."
HDFS_PATH="/user/amalia/energy_data/streaming"
docker exec namenode curl -s "http://localhost:9870/webhdfs/v1${HDFS_PATH}?op=LISTSTATUS" 2>&1 | python3 -m json.tool > /dev/null 2>&1
if [ $? -eq 0 ]; then
    echo "âœ… Hay datos en HDFS en $HDFS_PATH"
    echo "   Estructura:"
    docker exec namenode curl -s "http://localhost:9870/webhdfs/v1${HDFS_PATH}?op=LISTSTATUS" 2>&1 | python3 -c "
import json, sys
data = json.load(sys.stdin)
if 'FileStatuses' in data:
    for f in data['FileStatuses']['FileStatus'][:5]:
        print(f\"   - {f.get('pathSuffix', 'root')} ({f.get('type', 'UNKNOWN')})\")
" 2>&1
else
    echo "âš ï¸  No se encontraron datos en HDFS o hay un error de acceso"
fi
echo ""

# 8. Verificar Spark UI
echo "ðŸ“‹ 8. Verificando Spark UI..."
docker exec spark-consumer curl -s http://localhost:4040 > /dev/null 2>&1
if [ $? -eq 0 ]; then
    echo "âœ… Spark UI estÃ¡ accesible"
else
    echo "âš ï¸  Spark UI no estÃ¡ accesible (puede estar en otro puerto)"
    docker exec spark-consumer netstat -tlnp 2>/dev/null | grep 404
fi
echo ""

# 9. Verificar procesos Spark
echo "ðŸ“‹ 9. Verificando procesos Spark..."
docker exec spark-consumer ps aux | grep -E "spark|java.*SparkSubmit" | grep -v grep | head -3
echo ""

echo "=========================================="
echo "âœ… PRUEBA CAPA 4 COMPLETADA"
echo "=========================================="

