\documentclass[12pt,a4paper]{article}
% \usepackage{fontspec} % habilita UTF-8 nativo (solo XeLaTeX/LuaLaTeX)
\usepackage[utf8]{inputenc} % habilita UTF-8 para pdflatex
\usepackage[T1]{fontenc}    % codificación de fuente para pdflatex
\usepackage[spanish]{babel}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{float}
\usepackage{graphicx} % Par a insertar imágenes
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit}
\usepackage{listings}
\usepackage{xcolor}

\geometry{margin=2.5cm}
\setstretch{1.3}

\titleformat{\section}{\large\bfseries}{\thesection.}{1em}{}

% Configuración para código
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    showstringspaces=false
}

\begin{document}

%----------------------------------------------------------
% PORTADA
%----------------------------------------------------------
\begin{titlepage}
    \centering
    \vspace*{3cm}
    {\LARGE\textbf{Predicción de Consumo Energético en Ciudades Inteligentes}}\\[1cm]
    {\Large Proyecto de Procesamiento de Grandes Volúmenes de Datos}\\[2cm]
    {\large Autores:}\\[0.3cm]
    {\large Amalia Beatriz Valiente Hinojosa}\\
    {\large Noel Pérez Calvo}\\[1cm]    
    \vfill
\end{titlepage}

%----------------------------------------------------------
% CONTENIDO
%----------------------------------------------------------

\section{Introducción}

El objetivo central del proyecto es \textbf{predecir el consumo energético en distintas zonas de una ciudad inteligente}, utilizando técnicas de procesamiento de grandes volúmenes de datos mediante las plataformas \textit{Hadoop} y \textit{Apache Spark}.  
A través del análisis de datos históricos de consumo eléctrico, se busca anticipar la demanda en las próximas horas, detectar picos de consumo y evaluar el impacto de las condiciones climáticas sobre el uso energético urbano.

El sistema implementado es una arquitectura completa de streaming en tiempo real que procesa datos continuamente, los almacena de forma distribuida y proporciona visualización mediante un dashboard web interactivo.

\section{Dataset seleccionado}

\textbf{Nombre:} \textit{Household Electric Power Consumption Dataset} \\[0.2cm]
\textbf{Fuente:} Kaggle / UCI Machine Learning Repository \\[0.2cm]
\textbf{Formato:} CSV (valores separados por punto y coma ``;'') \\[0.2cm]
\textbf{URL:} \href{https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set}{https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set}

\subsection{Justificación del dataset}

\subsubsection*{Volumen}
El conjunto de datos contiene más de \textbf{dos millones de registros}, correspondientes a mediciones minuto a minuto del consumo eléctrico de un hogar durante casi \textbf{cuatro años} (2006–2010).  
Su tamaño aproximado de 120 MB en formato txt, y el incremento que supone su almacenamiento distribuido en \textit{HDFS}, permiten simular un entorno realista de \textbf{procesamiento de grandes volúmenes de datos}, adecuado para la aplicación de tecnologías como \textit{Hadoop} y \textit{Spark}.

\subsubsection*{Características}
El dataset incluye variables como:
\begin{itemize}
    \item Fecha y hora de cada registro.
    \item Potencia activa y reactiva global.
    \item Voltaje y corriente.
    \item Energía submedida en tres zonas del hogar (\textit{Sub\_metering\_1}, \textit{2} y \textit{3}).
\end{itemize}
Estas variables conforman una \textbf{serie temporal multivariable}, adecuada para tareas de predicción y análisis de patrones de consumo.  
Además, los datos presentan una variabilidad natural y cierto nivel de ruido, lo que refleja condiciones reales de consumo y permite evaluar técnicas de limpieza y modelado robustas.

\subsubsection*{Pertinencia}
El conjunto de datos resulta altamente pertinente para los objetivos del proyecto, ya que:
\begin{itemize}
    \item Contiene \textbf{registros reales de consumo energético}, directamente relacionados con la meta de predecir la demanda eléctrica.
    \item Su granularidad temporal (minuto a minuto) facilita el análisis de \textbf{patrones horarios, diarios y estacionales}.
    \item Puede combinarse con datos meteorológicos (temperatura, humedad, precipitaciones) para analizar el \textbf{impacto del clima} en la demanda, fortaleciendo el enfoque de ciudades inteligentes.
\end{itemize}

\section{Arquitectura del Sistema}

El sistema implementado sigue una arquitectura de streaming en tiempo real basada en microservicios contenedorizados. La arquitectura completa se compone de los siguientes componentes principales:

\subsection{Componentes del Sistema}

\begin{enumerate}
    \item \textbf{Producer (Python):} Aplicación que lee el dataset y envía datos a Kafka de forma continua.
    \item \textbf{Apache Kafka:} Sistema de mensajería distribuido que actúa como buffer entre el producer y el consumer.
    \item \textbf{Zookeeper:} Servicio de coordinación necesario para la gestión de Kafka.
    \item \textbf{Spark Consumer:} Aplicación Spark Structured Streaming que consume datos de Kafka, los transforma y los almacena en HDFS.
    \item \textbf{HDFS (Hadoop Distributed File System):} Sistema de almacenamiento distribuido donde se guardan los datos en formato Parquet.
    \item \textbf{Apache Hive:} Sistema de almacenamiento de datos que permite realizar consultas SQL sobre los datos almacenados en HDFS.
    \item \textbf{Dashboard Web:} Aplicación Flask que proporciona visualización en tiempo real de los datos mediante gráficos interactivos.
\end{enumerate}

\subsection{Flujo de Datos}

El flujo completo de datos sigue esta secuencia:

\begin{enumerate}
    \item El \textbf{Producer} lee registros del dataset cada 0.5 segundos y los envía a Kafka en formato JSON.
    \item \textbf{Kafka} almacena los mensajes en el tópico \texttt{energy\_stream}.
    \item El \textbf{Spark Consumer} lee los mensajes de Kafka cada 60 segundos (configurable) en micro-batches.
    \item Los datos se transforman agregando campos de particionado (año, mes, día, hora).
    \item Los datos transformados se escriben en \textbf{HDFS} en formato Parquet, particionados por año/mes/día/hora.
    \item \textbf{Hive} crea una tabla externa que apunta a los datos en HDFS, permitiendo consultas SQL.
    \item El \textbf{Dashboard} consulta los datos mediante Spark SQL y los visualiza en tiempo real.
\end{enumerate}

\subsection{Diagrama de Arquitectura}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{diagram.png}
    \caption{Arquitectura completa del sistema de procesamiento de datos energéticos.}
\end{figure}

\section{Fase 1: Implementación del Producer}

\subsection{Objetivo del Producer}

El objetivo del \textit{Producer} es emular la llegada de mediciones energéticas en tiempo real, transmitiendo registros de consumo eléctrico hacia Apache Kafka que actúe como capa intermedia entre las fuentes de datos y el almacenamiento distribuido en HDFS.

\subsection{Estructura de los Datos}

Los datos utilizados provienen de un conjunto de mediciones de energía con las siguientes variables:

\begin{itemize}
    \item \textbf{datetime}: Fecha y hora combinadas (formato \texttt{YYYY-MM-DD HH:MM:SS}).
    \item \textbf{global\_active\_power}: Potencia activa global (kW).
    \item \textbf{global\_reactive\_power}: Potencia reactiva global (kW).
    \item \textbf{voltage}: Voltaje promedio (V).
    \item \textbf{global\_intensity}: Intensidad de corriente (A).
    \item \textbf{sub\_metering\_1}: Consumo energético parcial del área 1 (Wh).
    \item \textbf{sub\_metering\_2}: Consumo energético parcial del área 2 (Wh).
    \item \textbf{sub\_metering\_3}: Consumo energético parcial del área 3 (Wh).
\end{itemize}

\subsection{Frecuencia de Envío}

El flujo de simulación definido envía una lectura cada \textbf{0.5 segundos}, representando un ritmo razonable para la actualización continua de datos de consumo en una red inteligente. Cada mensaje incluye un único registro en formato JSON con su respectivo sello temporal.

\subsection{Implementación}

El Producer está implementado en Python y se compone de los siguientes módulos:

\begin{itemize}
    \item \texttt{data\_loader.py}: Carga y limpia los datos del dataset CSV.
    \item \texttt{message\_builder.py}: Convierte los registros a formato JSON estructurado.
    \item \texttt{kafka\_client.py}: Maneja la conexión y envío de mensajes a Kafka.
    \item \texttt{config.py}: Configuración centralizada mediante variables de entorno.
    \item \texttt{producer.py}: Script principal que orquesta el flujo completo.
\end{itemize}

\section{Fase 2: Entorno Distribuido con Docker}

\subsection{Objetivo de la Fase}

El objetivo principal de esta fase es levantar un entorno funcional que permita la comunicación entre todos los componentes del sistema dentro de un entorno controlado y portable utilizando Docker y Docker Compose.

\subsection{Componentes del Entorno}

El ecosistema de servicios desplegado mediante Docker está compuesto por los siguientes contenedores:

\begin{itemize}
    \item \textbf{Zookeeper:} Servicio de coordinación y gestión de clústeres necesario para el correcto funcionamiento de Kafka.
    \item \textbf{Kafka Broker:} Sistema de mensajería distribuido encargado de recibir, almacenar y distribuir los mensajes provenientes del Producer.
    \item \textbf{NameNode:} Nodo maestro de HDFS que gestiona el espacio de nombres del sistema de archivos distribuido.
    \item \textbf{DataNode:} Nodo de almacenamiento de HDFS que almacena los bloques de datos.
    \item \textbf{ResourceManager:} Gestor de recursos de YARN para la gestión de recursos del clúster.
    \item \textbf{NodeManager:} Nodo trabajador de YARN que ejecuta las tareas.
    \item \textbf{Hive Metastore:} Servicio de metadatos que permite a Hive gestionar tablas y particiones.
    \item \textbf{Producer:} Aplicación Python que lee los datos del dataset y los envía a Kafka.
    \item \textbf{Spark Consumer:} Aplicación Spark que consume datos de Kafka y los almacena en HDFS.
    \item \textbf{Dashboard:} Aplicación web Flask para visualización de datos.
\end{itemize}

\subsection{Configuración de Servicios}

Cada servicio está configurado con:

\begin{itemize}
    \item Límites de memoria para evitar problemas de OOM (Out of Memory).
    \item Health checks para verificar el estado de los servicios.
    \item Políticas de reinicio automático (\texttt{restart: unless-stopped}).
    \item Variables de entorno configurables mediante archivo \texttt{.env}.
    \item Red interna Docker (\texttt{hadoop-net}) para comunicación entre servicios.
\end{itemize}

\section{Fase 3: Procesamiento con Apache Spark}

\subsection{Spark Structured Streaming}

El sistema utiliza \textbf{Spark Structured Streaming} para procesar los datos en tiempo real. Esta tecnología permite:

\begin{itemize}
    \item Procesamiento de streams continuos con latencia baja.
    \item Garantías de procesamiento exactly-once mediante checkpoints.
    \item Integración nativa con Kafka mediante el conector \texttt{spark-sql-kafka}.
    \item Escalabilidad horizontal automática.
\end{itemize}

\subsection{Transformación de Datos}

El consumer Spark realiza las siguientes transformaciones:

\begin{enumerate}
    \item \textbf{Lectura de Kafka:} Lee mensajes JSON del tópico \texttt{energy\_stream}.
    \item \textbf{Parsing:} Convierte los mensajes JSON a DataFrame estructurado.
    \item \textbf{Limpieza:} Valida y limpia los datos (elimina nulos, valida rangos).
    \item \textbf{Enriquecimiento:} Agrega campos de particionado (año, mes, día, hora) extraídos del timestamp.
    \item \textbf{Escritura:} Escribe los datos en HDFS en formato Parquet particionado.
\end{enumerate}

\subsection{Almacenamiento en HDFS}

Los datos se almacenan en HDFS con la siguiente estructura:

\begin{verbatim}
/user/{usuario}/{proyecto}/streaming/
  year=2006/
    month=12/
      day=20/
        hour=16/
          part-00000-xxx.snappy.parquet
\end{verbatim}

Esta estructura de particionado permite:

\begin{itemize}
    \item Consultas eficientes filtrando por fecha/hora.
    \item Escalabilidad horizontal del almacenamiento.
    \item Optimización de consultas mediante partition pruning.
\end{itemize}

\subsection{Formato Parquet}

El formato Parquet ofrece:

\begin{itemize}
    \item Compresión eficiente (hasta 10x de reducción de tamaño).
    \item Esquema embebido (tipo de datos preservado).
    \item Lectura columnar optimizada para análisis.
    \item Compatibilidad con múltiples herramientas (Spark, Hive, Pandas).
\end{itemize}

\section{Fase 4: Integración con Apache Hive}

\subsection{Tabla Externa de Hive}

Se crea una tabla externa en Hive que apunta a los datos almacenados en HDFS:

\begin{lstlisting}[language=SQL]
CREATE EXTERNAL TABLE IF NOT EXISTS energy_data (
    datetime TIMESTAMP,
    global_active_power DOUBLE,
    global_reactive_power DOUBLE,
    voltage DOUBLE,
    global_intensity DOUBLE,
    sub_metering_1 DOUBLE,
    sub_metering_2 DOUBLE,
    sub_metering_3 DOUBLE
)
PARTITIONED BY (year INT, month INT, day INT, hour INT)
STORED AS PARQUET
LOCATION 'hdfs://namenode:9000/user/amalia/energy_data/streaming';
\end{lstlisting}

\subsection{Ventajas de Hive}

\begin{itemize}
    \item Consultas SQL estándar sobre datos distribuidos.
    \item Detección automática de particiones nuevas.
    \item Integración con herramientas de BI y visualización.
    \item Optimización de consultas mediante particionado.
\end{itemize}

\section{Fase 5: Dashboard Web Interactivo}

\subsection{Implementación del Dashboard}

Se ha implementado un dashboard web moderno utilizando:

\begin{itemize}
    \item \textbf{Flask:} Framework web en Python para el backend.
    \item \textbf{Bootstrap 5:} Framework CSS para el diseño responsive.
    \item \textbf{Chart.js:} Biblioteca JavaScript para visualización de gráficos.
    \item \textbf{PySpark:} Para ejecutar consultas SQL sobre los datos.
\end{itemize}

\subsection{Funcionalidades del Dashboard}

El dashboard proporciona:

\begin{itemize}
    \item \textbf{Métricas en tiempo real:} Total de registros, promedios de potencia, voltaje e intensidad.
    \item \textbf{Gráficos de series temporales:} Visualización de consumo energético a lo largo del tiempo.
    \item \textbf{Agregados por hora:} Patrones de consumo horarios.
    \item \textbf{Distribución de sub-metering:} Gráfico de dona mostrando el consumo por zonas.
    \item \textbf{Tabla de últimos registros:} Visualización tabular de los datos más recientes.
\end{itemize}

\subsection{Actualización en Tiempo Real}

El dashboard se actualiza automáticamente cada 5 segundos mediante peticiones AJAX a los endpoints de la API REST, proporcionando una experiencia de visualización en tiempo real.

\subsection{Endpoints de la API}

\begin{itemize}
    \item \texttt{GET /api/statistics}: Retorna estadísticas agregadas del dataset.
    \item \texttt{GET /api/latest}: Retorna los últimos N registros.
    \item \texttt{GET /api/timeseries}: Retorna datos de series temporales.
    \item \texttt{GET /api/hourly}: Retorna agregados por hora.
    \item \texttt{GET /api/sub-metering}: Retorna distribución de sub-metering.
\end{itemize}

\section{Configuración y Personalización}

\subsection{Variables de Entorno}

El sistema está completamente configurable mediante variables de entorno definidas en el archivo \texttt{.env}:

\subsubsection*{Configuración de Kafka}
\begin{itemize}
    \item \texttt{KAFKA\_BROKER}: Dirección del broker de Kafka (default: \texttt{kafka:9092}).
    \item \texttt{KAFKA\_TOPIC}: Nombre del tópico (default: \texttt{energy\_stream}).
\end{itemize}

\subsubsection*{Configuración del Producer}
\begin{itemize}
    \item \texttt{PRODUCER\_SEND\_INTERVAL}: Intervalo entre envíos en segundos (default: \texttt{0.5}).
    \item \texttt{PRODUCER\_DATASET\_PATH}: Ruta al archivo del dataset.
    \item \texttt{PRODUCER\_MAX\_RETRIES}: Número máximo de reintentos en caso de error.
\end{itemize}

\subsubsection*{Configuración de HDFS}
\begin{itemize}
    \item \texttt{HDFS\_USER}: Usuario de HDFS (default: \texttt{amalia}).
    \item \texttt{HDFS\_GROUP}: Grupo de HDFS (default: \texttt{amalia}).
    \item \texttt{PROJECT\_NAME}: Nombre del proyecto (default: \texttt{energy\_data}).
    \item \texttt{HDFS\_NAMENODE}: Dirección del NameNode (default: \texttt{namenode}).
    \item \texttt{HDFS\_PORT}: Puerto de HDFS (default: \texttt{9000}).
\end{itemize}

\subsubsection*{Configuración de Spark}
\begin{itemize}
    \item \texttt{SPARK\_PROCESSING\_INTERVAL}: Intervalo de procesamiento en segundos (default: \texttt{60}).
    \item \texttt{SPARK\_CHECKPOINT\_LOCATION}: Ubicación de los checkpoints.
    \item \texttt{SPARK\_APP\_NAME}: Nombre de la aplicación Spark.
\end{itemize}

\subsubsection*{Configuración de Hive}
\begin{itemize}
    \item \texttt{HIVE\_METASTORE\_URI}: URI del metastore de Hive.
    \item \texttt{HIVE\_TABLE\_NAME}: Nombre de la tabla en Hive (default: \texttt{energy\_data}).
\end{itemize}

\subsubsection*{Configuración del Dashboard}
\begin{itemize}
    \item \texttt{DASHBOARD\_PORT}: Puerto del dashboard (default: \texttt{5001}).
    \item \texttt{DASHBOARD\_LATEST\_LIMIT}: Número de registros recientes a mostrar.
    \item \texttt{DASHBOARD\_TIMESERIES\_HOURS}: Horas de datos para series temporales.
\end{itemize}

\subsection{Ventajas de la Configuración Flexible}

\begin{itemize}
    \item Fácil adaptación a diferentes entornos (desarrollo, producción).
    \item No requiere modificar código para cambiar parámetros.
    \item Permite múltiples instancias del sistema con diferentes configuraciones.
    \item Facilita el mantenimiento y la escalabilidad.
\end{itemize}

\section{Optimizaciones y Mejoras Implementadas}

\subsection{Gestión de Memoria}

Se implementaron límites de memoria para todos los servicios críticos:

\begin{itemize}
    \item \textbf{NameNode:} 1GB máximo, 512MB reservado.
    \item \textbf{DataNode:} 512MB máximo, 256MB reservado.
    \item \textbf{Spark Consumer:} 2GB máximo, 1GB reservado.
    \item \textbf{Spark Driver/Executor:} 1GB cada uno para queries del dashboard.
\end{itemize}

Estos límites previenen problemas de Out of Memory (OOM) y garantizan la estabilidad del sistema.

\subsection{Reinicio Automático}

Todos los servicios están configurados con \texttt{restart: unless-stopped}, lo que garantiza que:

\begin{itemize}
    \item Los servicios se reinician automáticamente si fallan.
    \item El sistema se recupera automáticamente de errores transitorios.
    \item Se mantiene la alta disponibilidad del sistema.
\end{itemize}

\subsection{Health Checks}

Cada servicio crítico tiene health checks configurados:

\begin{itemize}
    \item \textbf{Kafka:} Verifica que el broker esté respondiendo.
    \item \textbf{NameNode:} Verifica que la interfaz web esté disponible.
    \item \textbf{DataNode:} Verifica el reporte de estado de HDFS.
\end{itemize}

Los servicios dependientes esperan a que los servicios base estén saludables antes de iniciar.

\subsection{Procesamiento Exactly-Once}

El sistema garantiza procesamiento exactly-once mediante:

\begin{itemize}
    \item Checkpoints de Spark Structured Streaming.
    \item Offsets de Kafka gestionados automáticamente.
    \item Transacciones atómicas en la escritura a HDFS.
\end{itemize}

\section{Interfaces Web Disponibles}

El sistema proporciona las siguientes interfaces web para monitoreo y visualización:

\begin{itemize}
    \item \textbf{HDFS NameNode UI:} \texttt{http://localhost:9870} - Explorador de archivos HDFS y métricas del sistema.
    \item \textbf{Spark UI:} \texttt{http://localhost:4040} - Monitoreo de jobs Spark, stages y tareas.
    \item \textbf{YARN ResourceManager UI:} \texttt{http://localhost:8088} - Gestión de recursos y aplicaciones.
    \item \textbf{Dashboard:} \texttt{http://localhost:5001} - Visualización interactiva de datos en tiempo real.
\end{itemize}

\section{Resultados y Funcionalidades}

\subsection{Datos Procesados}

El sistema procesa exitosamente:

\begin{itemize}
    \item Más de 2 millones de registros del dataset histórico.
    \item Streaming continuo de datos en tiempo real.
    \item Almacenamiento eficiente en formato Parquet comprimido.
    \item Consultas SQL rápidas mediante Hive sobre datos particionados.
\end{itemize}

\subsection{Métricas del Sistema}

\begin{itemize}
    \item \textbf{Velocidad de ingesta:} 1 registro cada 0.5 segundos (120 registros/minuto).
    \item \textbf{Velocidad de procesamiento:} Micro-batches cada 60 segundos.
    \item \textbf{Compresión:} Reducción de tamaño ~10x con formato Parquet.
    \item \textbf{Particionado:} Por año, mes, día y hora para optimización de consultas.
\end{itemize}

\subsection{Escalabilidad}

El sistema está diseñado para ser escalable:

\begin{itemize}
    \item Kafka puede escalar horizontalmente agregando más brokers.
    \item Spark puede procesar múltiples streams en paralelo.
    \item HDFS puede agregar más DataNodes para aumentar capacidad.
    \item El dashboard puede servir múltiples usuarios concurrentes.
\end{itemize}

\section{Conclusión}

Se ha implementado exitosamente un sistema completo de procesamiento de grandes volúmenes de datos para predicción de consumo energético. El sistema integra:

\begin{itemize}
    \item Ingesta de datos en tiempo real mediante Kafka.
    \item Procesamiento distribuido con Apache Spark.
    \item Almacenamiento escalable en HDFS con formato Parquet.
    \item Consultas SQL mediante Apache Hive.
    \item Visualización interactiva mediante dashboard web.
\end{itemize}

El sistema es robusto, escalable y completamente funcional, proporcionando una base sólida para el análisis y predicción de consumo energético en ciudades inteligentes. La arquitectura implementada permite el procesamiento continuo de datos, el almacenamiento eficiente y la visualización en tiempo real, cumpliendo con todos los objetivos planteados inicialmente.

\end{document}
